{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GameNet (Hartford et al 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string as string\n",
    "from __future__ import division\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import_csv = pd.read_csv('gamesmxn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine same games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, import_csv.shape[0]):\n",
    "    if pd.DataFrame(import_csv['matrixrow'][0:i] == import_csv['matrixrow'][i]).values.any():\n",
    "        repeatmat = import_csv['matrixrow'][0:i][import_csv['matrixrow'][0:i] == import_csv['matrixrow'][i]].index[0]\n",
    "        choicesum = np.matrix(import_csv['choicerow'][repeatmat]) + np.matrix(import_csv['choicerow'][i])\n",
    "        choicesum = string.replace(str(choicesum),'[','')\n",
    "        choicesum = string.replace(str(choicesum),']]','')\n",
    "        choicesum = string.replace(str(choicesum),']\\n',';')\n",
    "        import_csv.set_value(repeatmat, 'choicerow', choicesum)\n",
    "        import_csv.drop([i], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select games to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = np.empty((import_csv.shape[0]))\n",
    "tmp[:] = np.NAN\n",
    "for i in range(import_csv.shape[0]):\n",
    "    if (import_csv['shape'][i] == '3 3' \n",
    "        and import_csv['symmetric'][i] == 1 \n",
    "        and import_csv['paper'][i] != 'stahlwilson1995'):\n",
    "        tmp[i] = i\n",
    "index = tmp[~np.isnan(tmp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs_row = np.zeros((index.shape[0], 3, 3, 2))\n",
    "inputs_col = np.zeros((index.shape[0], 3, 3, 2))\n",
    "target_row = np.zeros((index.shape[0], 3, 1))\n",
    "for j in range(index.shape[0]):\n",
    "    Ur = np.matrix(import_csv['matrixrow'][int(index[j])])\n",
    "    Ur = (Ur-np.mean(Ur))/np.std(Ur)\n",
    "    Uc = np.transpose(Ur)\n",
    "    ar = np.matrix(import_csv['choicerow'][int(index[j])])\n",
    "    if ar.shape[1] == 2:\n",
    "        ar = ar[:,0]/ar[:,1]\n",
    "    inputs_row[j, :, :, 0] = Ur\n",
    "    inputs_row[j, :, :, 1] = Uc\n",
    "    inputs_col[j, :, :, 0] = Uc\n",
    "    inputs_col[j, :, :, 1] = Ur\n",
    "    target_row[j] = ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(range(inputs_row.shape[0]))\n",
    "index_train = shuffle[0:inputs_row.shape[0]//5*4] \n",
    "index_tests = shuffle[inputs_row.shape[0]//5*4:inputs_row.shape[0]]\n",
    "inputs_row_train = inputs_row[index_train]\n",
    "inputs_col_train = inputs_col[index_train]\n",
    "target_row_train = target_row[index_train]\n",
    "inputs_row_tests = inputs_row[index_tests]\n",
    "inputs_col_tests = inputs_col[index_tests]\n",
    "target_row_tests = target_row[index_tests]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolution and pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rw_pool(x,c):\n",
    "    x_max = tf.reduce_max(x, axis=2)\n",
    "    x_til = tf.tile(x_max,[1,3,1])\n",
    "    x_sha = tf.reshape(x_til,[-1,3,3,c])\n",
    "    return tf.transpose(x_sha, perm=[0,2,1,3])\n",
    "\n",
    "def cw_pool(x,c):\n",
    "    x_max = tf.reduce_max(x, axis=1)\n",
    "    x_til = tf.tile(x_max,[1,3,1])\n",
    "    return tf.reshape(x_til,[-1,3,3,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_row = tf.placeholder(tf.float32, shape=[None, 3, 3, 2])\n",
    "x_col = tf.placeholder(tf.float32, shape=[None, 3, 3, 2])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer 1 (row player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow version `0.12.1` uses `tf.concat(axis, values, name='concat')`, not `tf.concat(values, axis, name='concat')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pool1 = tf.concat(3, [x_row, rw_pool(x_row, 2), cw_pool(x_row, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([1, 1, 6, 50])\n",
    "b_conv1 = bias_variable([50])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_pool1, W_conv1) + b_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer 2 (row player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pool2 = tf.concat(3, [h_conv1, rw_pool(h_conv1, 50), cw_pool(h_conv1, 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([1, 1, 150, 50])\n",
    "b_conv2 = bias_variable([50])\n",
    "h_conv2 = tf.nn.relu(conv2d(x_pool2, W_conv2) + b_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_conv2_drop = tf.nn.dropout(h_conv2, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <h3> Hidden layer 1 (col player) </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pool1_col = tf.concat(3, [x_col, rw_pool(x_col, 2), cw_pool(x_col, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv1_col = weight_variable([1, 1, 6, 50])\n",
    "b_conv1_col = bias_variable([50])\n",
    "h_conv1_col = tf.nn.relu(conv2d(x_pool1_col, W_conv1_col) + b_conv1_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <h3> Hidden layer 2 (col player) </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_pool2_col = tf.concat(3, [h_conv1_col, rw_pool(h_conv1_col, 50), cw_pool(h_conv1_col, 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv2_col = weight_variable([1, 1, 150, 50])\n",
    "b_conv2_col = bias_variable([50])\n",
    "h_conv2_col = tf.nn.relu(conv2d(x_pool2_col, W_conv2_col) + b_conv2_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob_col = tf.placeholder(tf.float32)\n",
    "h_conv2_col_drop = tf.nn.dropout(h_conv2_col, keep_prob_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <h3> Action response layer 0 (col player) </h3> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_rwsm0 = weight_variable([1, 1, 50, 50])\n",
    "h_rdsm1 = conv2d(h_conv2_col_drop, W_rwsm0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ar_rwsm0_col = tf.reduce_sum(h_conv2_col_drop, axis=2)\n",
    "ar_sfmx0_col = tf.nn.softmax(ar_rwsm0_col, dim=1)\n",
    "ar_sfmx0_col = tf.slice(ar_sfmx0_col, [0, 0, 0], [-1, 3, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action response layer 1 (row player response to col player)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_rdsm1 = weight_variable([1, 1, 50, 50])\n",
    "h_rdsm1 = conv2d(h_conv2_drop, W_rdsm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar_rdsm1 = tf.reduce_sum(h_rdsm1, axis=3)\n",
    "ar_dtpt1 = tf.matmul(ar_rdsm1, ar_sfmx0_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(ar_dtpt1, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.01\n",
    "regularizer = (tf.nn.l2_loss(W_conv1) \n",
    "               + tf.nn.l2_loss(W_conv2) \n",
    "               + tf.nn.l2_loss(W_conv1_col) \n",
    "               + tf.nn.l2_loss(W_conv2_col)\n",
    "               + tf.nn.l2_loss(W_rwsm0) \n",
    "               + tf.nn.l2_loss(W_rdsm1))\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y) + beta * regularizer, reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.0002,0.9,0.999,1e-8).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0, train NLL 1.21286\n",
      "step 100, train accuracy 1, train NLL 0.931282\n",
      "step 200, train accuracy 0.666667, train NLL 0.923004\n",
      "step 300, train accuracy 1, train NLL 0.991581\n",
      "step 400, train accuracy 1, train NLL 0.870733\n",
      "step 500, train accuracy 1, train NLL 0.909504\n",
      "step 600, train accuracy 1, train NLL 0.970927\n",
      "step 700, train accuracy 1, train NLL 0.791203\n",
      "step 800, train accuracy 1, train NLL 0.900529\n",
      "step 900, train accuracy 1, train NLL 0.768933\n",
      "step 1000, train accuracy 1, train NLL 1.0487\n",
      "step 1100, train accuracy 1, train NLL 0.98499\n",
      "step 1200, train accuracy 1, train NLL 0.943656\n",
      "step 1300, train accuracy 1, train NLL 0.964847\n",
      "step 1400, train accuracy 1, train NLL 0.822732\n",
      "step 1500, train accuracy 1, train NLL 0.756436\n",
      "step 1600, train accuracy 1, train NLL 0.841448\n",
      "step 1700, train accuracy 1, train NLL 0.988413\n",
      "step 1800, train accuracy 1, train NLL 0.936523\n",
      "step 1900, train accuracy 1, train NLL 0.818463\n",
      "step 2000, train accuracy 1, train NLL 0.923362\n",
      "step 2100, train accuracy 1, train NLL 0.737625\n",
      "step 2200, train accuracy 1, train NLL 0.918788\n",
      "step 2300, train accuracy 1, train NLL 0.916267\n",
      "step 2400, train accuracy 1, train NLL 0.80435\n",
      "step 2500, train accuracy 1, train NLL 0.899768\n",
      "step 2600, train accuracy 1, train NLL 0.932072\n",
      "step 2700, train accuracy 1, train NLL 0.952133\n",
      "step 2800, train accuracy 1, train NLL 0.882721\n",
      "step 2900, train accuracy 1, train NLL 0.932252\n",
      "step 3000, train accuracy 1, train NLL 0.891669\n",
      "step 3100, train accuracy 1, train NLL 0.680897\n",
      "step 3200, train accuracy 1, train NLL 0.987413\n",
      "step 3300, train accuracy 1, train NLL 0.727582\n",
      "step 3400, train accuracy 1, train NLL 0.878936\n",
      "step 3500, train accuracy 1, train NLL 0.92533\n",
      "step 3600, train accuracy 1, train NLL 0.925362\n",
      "step 3700, train accuracy 1, train NLL 0.972061\n",
      "step 3800, train accuracy 1, train NLL 1.01988\n",
      "step 3900, train accuracy 1, train NLL 0.786818\n",
      "step 4000, train accuracy 1, train NLL 1.04481\n",
      "step 4100, train accuracy 1, train NLL 0.740391\n",
      "step 4200, train accuracy 1, train NLL 0.828346\n",
      "step 4300, train accuracy 1, train NLL 0.888955\n",
      "step 4400, train accuracy 1, train NLL 0.929829\n",
      "step 4500, train accuracy 1, train NLL 0.755884\n",
      "step 4600, train accuracy 1, train NLL 0.907264\n",
      "step 4700, train accuracy 1, train NLL 0.786961\n",
      "step 4800, train accuracy 1, train NLL 0.690313\n",
      "step 4900, train accuracy 1, train NLL 1.04828\n"
     ]
    }
   ],
   "source": [
    "for i in range(5000):\n",
    "    if i%5 == 0:\n",
    "        shuffle = np.random.permutation(range(inputs_row_train.shape[0]))\n",
    "    inputs_row_train_batch = inputs_row_train[shuffle[shuffle.shape[0]//5*(i%5):shuffle.shape[0]//5*(i%5+1)]]\n",
    "    inputs_col_train_batch = inputs_col_train[shuffle[shuffle.shape[0]//5*(i%5):shuffle.shape[0]//5*(i%5+1)]]\n",
    "    target_row_train_batch = target_row_train[shuffle[shuffle.shape[0]//5*(i%5):shuffle.shape[0]//5*(i%5+1)]]\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={\n",
    "                x_row: inputs_row_train_batch,\n",
    "                x_col: inputs_col_train_batch,\n",
    "                y_: target_row_train_batch, \n",
    "                keep_prob: 1.0,\n",
    "                keep_prob_col: 1.0})\n",
    "        train_NLL = cross_entropy.eval(feed_dict={\n",
    "                x_row: inputs_row_train_batch, \n",
    "                x_col: inputs_col_train_batch,\n",
    "                y_: target_row_train_batch,\n",
    "                keep_prob: 1.0,\n",
    "                keep_prob_col: 1.0})\n",
    "        print(\"step %d, train accuracy %g, train NLL %g\"%(i, train_accuracy, train_NLL))\n",
    "    train_step.run(feed_dict={x_row: inputs_row_train_batch, \n",
    "                              x_col: inputs_col_train_batch, \n",
    "                              y_: target_row_train_batch, \n",
    "                              keep_prob: 0.8,\n",
    "                              keep_prob_col: 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.714286, test NLL 0.896243\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy.eval(feed_dict={\n",
    "        x_row: inputs_row_tests,\n",
    "        x_col: inputs_col_tests,\n",
    "        y_: target_row_tests, \n",
    "        keep_prob: 1.0,\n",
    "        keep_prob_col: 1.0})\n",
    "test_NLL = cross_entropy.eval(feed_dict={\n",
    "        x_row: inputs_row_tests,\n",
    "        x_col: inputs_col_tests,\n",
    "        y_: target_row_tests, \n",
    "        keep_prob: 1.0,\n",
    "        keep_prob_col: 1.0})\n",
    "print(\"test accuracy %g, test NLL %g\"%(test_accuracy, test_NLL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301426</td>\n",
       "      <td>0.222910</td>\n",
       "      <td>0.475664</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191137</td>\n",
       "      <td>0.643386</td>\n",
       "      <td>0.165477</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.204796</td>\n",
       "      <td>0.163544</td>\n",
       "      <td>0.631660</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.716834</td>\n",
       "      <td>0.089620</td>\n",
       "      <td>0.193546</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.486653</td>\n",
       "      <td>0.410669</td>\n",
       "      <td>0.102678</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.319728</td>\n",
       "      <td>0.339683</td>\n",
       "      <td>0.340588</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.721474</td>\n",
       "      <td>0.194766</td>\n",
       "      <td>0.083761</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         y1        y2        y3    y_1    y_2    y_3\n",
       "0  0.301426  0.222910  0.475664  0.125  0.000  0.875\n",
       "1  0.191137  0.643386  0.165477  0.100  0.875  0.025\n",
       "2  0.204796  0.163544  0.631660  0.275  0.000  0.725\n",
       "3  0.716834  0.089620  0.193546  0.650  0.000  0.350\n",
       "4  0.486653  0.410669  0.102678  0.000  0.675  0.325\n",
       "5  0.319728  0.339683  0.340588  0.440  0.260  0.300\n",
       "6  0.721474  0.194766  0.083761  0.610  0.350  0.040"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "compare_y = y.eval(feed_dict={\n",
    "                    x_row: inputs_row_tests, \n",
    "                    x_col: inputs_col_tests, \n",
    "                    y_: target_row_tests, \n",
    "                    keep_prob: 1.0, \n",
    "                    keep_prob_col: 1.0})\n",
    "compare_y = compare_y.reshape(compare_y.shape[0], compare_y.shape[1])\n",
    "compare_y_ = y_.eval(feed_dict={\n",
    "                        x_row: inputs_row_tests,\n",
    "                        x_col: inputs_col_tests,\n",
    "                        y_: target_row_tests, \n",
    "                        keep_prob: 1.0,\n",
    "                        keep_prob_col: 1.0})\n",
    "compare_y_ = compare_y_.reshape(compare_y_.shape[0], compare_y_.shape[1])\n",
    "compare = pd.DataFrame(np.concatenate((compare_y, compare_y_), axis=1))\n",
    "compare.columns = [\"y1\",\"y2\",\"y3\",\"y_1\",\"y_2\",\"y_3\"]\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
