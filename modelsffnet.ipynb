{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNet (Hartford et al 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_csv = pd.read_csv('gamesmxn.csv')\n",
    "inputs_csv = np.zeros((import_csv.shape[0],18))\n",
    "target_csv = np.zeros((import_csv.shape[0],3))\n",
    "for i in range(import_csv.shape[0]):\n",
    "    if import_csv['shape'][i] == '3 3' and import_csv['symmetric'][i] == 1:\n",
    "        Ur = np.matrix(import_csv['matrixrow'][i])\n",
    "        Ur = (Ur-np.mean(Ur))/np.std(Ur)\n",
    "        Ur_vector = np.array(Ur).flatten()\n",
    "        Uc = np.transpose(Ur)\n",
    "        Uc_vector = np.array(Uc).flatten()\n",
    "        inputs_csv[i] = np.concatenate((Ur_vector, Uc_vector), axis=0)\n",
    "        ar = np.matrix(import_csv['choicerow'][i])\n",
    "        if ar.shape[1] == 2:\n",
    "            ar = ar[:,0]/ar[:,1]\n",
    "        ar_vector = ar.flatten()\n",
    "        target_csv[i] = ar_vector\n",
    "inputs_set = inputs_csv[~np.all(inputs_csv == 0, axis=1)]\n",
    "target_set = target_csv[~np.all(inputs_csv == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = [[[1,0,0],[0,1,0],[0,0,1]],[[1,0,0],[0,0,1],[0,1,0]],\n",
    "     [[0,1,0],[1,0,0],[0,0,1]],[[0,1,0],[0,0,1],[1,0,0]],\n",
    "     [[0,0,1],[1,0,0],[0,1,0]],[[0,0,1],[0,1,0],[1,0,0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_aug = np.zeros((inputs_set.shape[0]*6*6,18))\n",
    "target_aug = np.zeros((inputs_set.shape[0]*6*6,3))\n",
    "for k in range(6):\n",
    "    for i in range(inputs_set.shape[0]*6)[::6]:\n",
    "        Ur = p[k]*np.matrix(inputs_set[i//6,0:9]).reshape(3,3)\n",
    "        Ur = (Ur-np.mean(Ur))/np.std(Ur)\n",
    "        for j in range(6):\n",
    "            Ur = Ur*p[j]\n",
    "            Ur_vector = np.array(Ur).flatten()\n",
    "            Uc = np.transpose(Ur)\n",
    "            Uc = Uc*p[j]\n",
    "            Uc_vector = np.array(Uc).flatten()\n",
    "            inputs_aug[k*inputs_set.shape[0]*6+i+j] = np.concatenate((Ur_vector, Uc_vector), axis=0)\n",
    "            ar = np.matrix(target_set[i//6]).reshape(3,1)\n",
    "            ar = p[k]*ar\n",
    "            ar_vector = ar.flatten()\n",
    "            target_aug[k*inputs_set.shape[0]*6+i+j] = ar_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs_train = inputs_aug\n",
    "target_train = target_aug\n",
    "inputs_tests = inputs_set\n",
    "target_tests = target_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 18])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hi = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_1 = weight_variable([18, hi])\n",
    "b_1 = bias_variable([hi])\n",
    "h_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_2 = weight_variable([hi, hi])\n",
    "b_2 = bias_variable([hi])\n",
    "h_2 = tf.nn.relu(tf.matmul(h_1, W_2) + b_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "- Drop probability = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_2_drop = tf.nn.dropout(h_2, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_3 = weight_variable([hi, 3])\n",
    "b_3 = bias_variable([3])\n",
    "y = tf.nn.softmax(tf.matmul(h_2_drop, W_3) + b_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $L_2$ regularization \n",
    "- $ \\beta = 0.01 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "beta = 0.01\n",
    "regularizer = tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2) + tf.nn.l2_loss(W_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y) + beta * regularizer, reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adam\n",
    "- initial learning rate = $ 0.0002 $\n",
    "- $ \\beta_1 = 0.9 $\n",
    "- $ \\beta_2 = 0.999 $\n",
    "- $ \\epsilon = 1e-8 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(0.0002,0.9,0.999,1e-8).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.265432, train NLL 0.699599\n",
      "step 100, train accuracy 0.523148, train NLL 0.499286\n",
      "step 200, train accuracy 0.625, train NLL 0.203482\n",
      "step 300, train accuracy 0.736111, train NLL -0.16775\n",
      "step 400, train accuracy 0.768519, train NLL -0.590908\n",
      "step 500, train accuracy 0.746914, train NLL -1.01584\n",
      "step 600, train accuracy 0.762346, train NLL -1.49931\n",
      "step 700, train accuracy 0.799383, train NLL -2.07745\n",
      "step 800, train accuracy 0.759259, train NLL -2.70878\n",
      "step 900, train accuracy 0.773148, train NLL -3.44219\n",
      "step 1000, train accuracy 0.774691, train NLL -4.26238\n",
      "step 1100, train accuracy 0.782407, train NLL -5.1589\n",
      "step 1200, train accuracy 0.75463, train NLL -6.12547\n",
      "step 1300, train accuracy 0.734568, train NLL -7.20263\n",
      "step 1400, train accuracy 0.785494, train NLL -8.35143\n",
      "step 1500, train accuracy 0.74537, train NLL -9.57695\n",
      "step 1600, train accuracy 0.734568, train NLL -10.8969\n",
      "step 1700, train accuracy 0.768519, train NLL -12.2762\n",
      "step 1800, train accuracy 0.78858, train NLL -13.7502\n",
      "step 1900, train accuracy 0.737654, train NLL -15.2725\n",
      "step 2000, train accuracy 0.765432, train NLL -16.8937\n",
      "step 2100, train accuracy 0.746914, train NLL -18.5639\n",
      "step 2200, train accuracy 0.728395, train NLL -20.283\n",
      "step 2300, train accuracy 0.746914, train NLL -22.08\n",
      "step 2400, train accuracy 0.75, train NLL -23.9891\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "index_shuffle = np.random.permutation(range(inputs_train.shape[0]))\n",
    "for i in range(2500):\n",
    "    if i%6 == 0:\n",
    "        index_shuffle = np.random.permutation(range(inputs_train.shape[0]))\n",
    "    inputs_train_batch = inputs_train[index_shuffle[index_shuffle.shape[0]//6*(i%6):index_shuffle.shape[0]//6*(i%6+1)]]\n",
    "    target_train_batch = target_train[index_shuffle[index_shuffle.shape[0]//6*(i%6):index_shuffle.shape[0]//6*(i%6+1)]]\n",
    "    if i%100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: inputs_train_batch, y_: target_train_batch, keep_prob: 1.0})\n",
    "        train_NLL = cross_entropy.eval(feed_dict={x: inputs_train_batch, y_: target_train_batch, keep_prob: 1.0})\n",
    "        print(\"step %d, train accuracy %g, train NLL %g\"%(i, train_accuracy, train_NLL))\n",
    "    train_step.run(feed_dict={x: inputs_train_batch, y_: target_train_batch, keep_prob: 0.8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy 0.759259, test NLL -25.8901\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy.eval(feed_dict={x: inputs_tests, y_: target_tests, keep_prob: 1.0})\n",
    "test_NLL = cross_entropy.eval(feed_dict={x: inputs_tests, y_: target_tests, keep_prob: 1.0})\n",
    "print(\"test accuracy %g, test NLL %g\"%(test_accuracy, test_NLL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "      <th>y_1</th>\n",
       "      <th>y_2</th>\n",
       "      <th>y_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.231543</td>\n",
       "      <td>0.205917</td>\n",
       "      <td>0.562540</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.392368</td>\n",
       "      <td>0.269764</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.443386</td>\n",
       "      <td>0.515899</td>\n",
       "      <td>0.040715</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029966</td>\n",
       "      <td>0.591738</td>\n",
       "      <td>0.378297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.456095</td>\n",
       "      <td>0.181239</td>\n",
       "      <td>0.362666</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.242409</td>\n",
       "      <td>0.592754</td>\n",
       "      <td>0.164837</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.043063</td>\n",
       "      <td>0.406220</td>\n",
       "      <td>0.550717</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.266536</td>\n",
       "      <td>0.318728</td>\n",
       "      <td>0.414736</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.686134</td>\n",
       "      <td>0.156928</td>\n",
       "      <td>0.156938</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.481345</td>\n",
       "      <td>0.007312</td>\n",
       "      <td>0.511343</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.411049</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.278421</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.182504</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>0.473326</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.161659</td>\n",
       "      <td>0.352787</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.279701</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>0.385038</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.354167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.448748</td>\n",
       "      <td>0.205368</td>\n",
       "      <td>0.345883</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.042217</td>\n",
       "      <td>0.631844</td>\n",
       "      <td>0.325939</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.741335</td>\n",
       "      <td>0.099506</td>\n",
       "      <td>0.159158</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.282196</td>\n",
       "      <td>0.170358</td>\n",
       "      <td>0.547446</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.573267</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.425725</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.645833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.285907</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.347581</td>\n",
       "      <td>0.151866</td>\n",
       "      <td>0.500553</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.410772</td>\n",
       "      <td>0.560348</td>\n",
       "      <td>0.028880</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.446779</td>\n",
       "      <td>0.386421</td>\n",
       "      <td>0.166800</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.181623</td>\n",
       "      <td>0.344440</td>\n",
       "      <td>0.473937</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.161659</td>\n",
       "      <td>0.352787</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.280048</td>\n",
       "      <td>0.336158</td>\n",
       "      <td>0.383794</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.449181</td>\n",
       "      <td>0.205641</td>\n",
       "      <td>0.345179</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.628061</td>\n",
       "      <td>0.330349</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.742360</td>\n",
       "      <td>0.098499</td>\n",
       "      <td>0.159141</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.280363</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.543789</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.574079</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.424953</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.283973</td>\n",
       "      <td>0.148289</td>\n",
       "      <td>0.567737</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.411049</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>0.568965</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.278421</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.206897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.311129</td>\n",
       "      <td>0.387039</td>\n",
       "      <td>0.301832</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.258621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.370784</td>\n",
       "      <td>0.221425</td>\n",
       "      <td>0.407791</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.327586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.485555</td>\n",
       "      <td>0.161659</td>\n",
       "      <td>0.352787</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.431034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.279701</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>0.385038</td>\n",
       "      <td>0.534483</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.327586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.454607</td>\n",
       "      <td>0.233683</td>\n",
       "      <td>0.311709</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.379310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.182504</td>\n",
       "      <td>0.344171</td>\n",
       "      <td>0.473326</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.706897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.448748</td>\n",
       "      <td>0.205368</td>\n",
       "      <td>0.345883</td>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.241379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.308948</td>\n",
       "      <td>0.450920</td>\n",
       "      <td>0.240131</td>\n",
       "      <td>0.258621</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.228571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.741335</td>\n",
       "      <td>0.099506</td>\n",
       "      <td>0.159158</td>\n",
       "      <td>0.603448</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.344828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.282196</td>\n",
       "      <td>0.170358</td>\n",
       "      <td>0.547446</td>\n",
       "      <td>0.224138</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.603448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.573267</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.425725</td>\n",
       "      <td>0.568965</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.396552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.285907</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.293103</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.672414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.602258</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>0.088986</td>\n",
       "      <td>0.789116</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.163265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.245639</td>\n",
       "      <td>0.241903</td>\n",
       "      <td>0.512459</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.925170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.411049</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.040816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.508040</td>\n",
       "      <td>0.244555</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.054422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.830847</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.115119</td>\n",
       "      <td>0.911565</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.061224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.285907</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>0.748299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.576628</td>\n",
       "      <td>0.192186</td>\n",
       "      <td>0.231186</td>\n",
       "      <td>0.843537</td>\n",
       "      <td>0.034014</td>\n",
       "      <td>0.122449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.254107</td>\n",
       "      <td>0.727788</td>\n",
       "      <td>0.018105</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.027211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.054422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.284550</td>\n",
       "      <td>0.188365</td>\n",
       "      <td>0.527085</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.931973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.481115</td>\n",
       "      <td>0.502597</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.020408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.573267</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.425725</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.088435</td>\n",
       "      <td>0.258503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.602591</td>\n",
       "      <td>0.178472</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>0.693878</td>\n",
       "      <td>0.102041</td>\n",
       "      <td>0.204082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.048255</td>\n",
       "      <td>0.710317</td>\n",
       "      <td>0.241427</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.870748</td>\n",
       "      <td>0.102041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.278421</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.183673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.444206</td>\n",
       "      <td>0.236482</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.748299</td>\n",
       "      <td>0.149660</td>\n",
       "      <td>0.102041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.204632</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.535353</td>\n",
       "      <td>0.027211</td>\n",
       "      <td>0.068027</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.279701</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>0.385038</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>0.319728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.335557</td>\n",
       "      <td>0.628287</td>\n",
       "      <td>0.036156</td>\n",
       "      <td>0.197279</td>\n",
       "      <td>0.659864</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.102947</td>\n",
       "      <td>0.354474</td>\n",
       "      <td>0.542579</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.074830</td>\n",
       "      <td>0.911565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.325120</td>\n",
       "      <td>0.601401</td>\n",
       "      <td>0.073479</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>0.653333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.023402</td>\n",
       "      <td>0.549526</td>\n",
       "      <td>0.427073</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.595773</td>\n",
       "      <td>0.346056</td>\n",
       "      <td>0.058171</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.602591</td>\n",
       "      <td>0.178472</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.053333</td>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.980008</td>\n",
       "      <td>0.005735</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.546667</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.362044</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.637378</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.506667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.190147</td>\n",
       "      <td>0.404341</td>\n",
       "      <td>0.405512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.462945</td>\n",
       "      <td>0.244622</td>\n",
       "      <td>0.292433</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.053333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.137816</td>\n",
       "      <td>0.277193</td>\n",
       "      <td>0.584992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.335557</td>\n",
       "      <td>0.628287</td>\n",
       "      <td>0.036156</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.693333</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.631657</td>\n",
       "      <td>0.086628</td>\n",
       "      <td>0.281715</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.370784</td>\n",
       "      <td>0.221425</td>\n",
       "      <td>0.407791</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.186667</td>\n",
       "      <td>0.293333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.165873</td>\n",
       "      <td>0.365628</td>\n",
       "      <td>0.468499</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.573333</td>\n",
       "      <td>0.413333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.448150</td>\n",
       "      <td>0.435468</td>\n",
       "      <td>0.116382</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.586667</td>\n",
       "      <td>0.013333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>0.711729</td>\n",
       "      <td>0.257204</td>\n",
       "      <td>0.031067</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>0.126751</td>\n",
       "      <td>0.426659</td>\n",
       "      <td>0.446591</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.382979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.554718</td>\n",
       "      <td>0.053493</td>\n",
       "      <td>0.391789</td>\n",
       "      <td>0.276596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.247404</td>\n",
       "      <td>0.508040</td>\n",
       "      <td>0.244555</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.576628</td>\n",
       "      <td>0.192186</td>\n",
       "      <td>0.231186</td>\n",
       "      <td>0.829787</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.148936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.284550</td>\n",
       "      <td>0.188365</td>\n",
       "      <td>0.527085</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.602591</td>\n",
       "      <td>0.178472</td>\n",
       "      <td>0.218937</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.191489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.444206</td>\n",
       "      <td>0.236482</td>\n",
       "      <td>0.319312</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.245639</td>\n",
       "      <td>0.241903</td>\n",
       "      <td>0.512459</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085106</td>\n",
       "      <td>0.914894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.830847</td>\n",
       "      <td>0.054034</td>\n",
       "      <td>0.115119</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.254107</td>\n",
       "      <td>0.727788</td>\n",
       "      <td>0.018105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.978723</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.481115</td>\n",
       "      <td>0.502597</td>\n",
       "      <td>0.016288</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.340426</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.048255</td>\n",
       "      <td>0.710317</td>\n",
       "      <td>0.241427</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.106383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.204632</td>\n",
       "      <td>0.260015</td>\n",
       "      <td>0.535353</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.102947</td>\n",
       "      <td>0.354474</td>\n",
       "      <td>0.542579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.602258</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>0.088986</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.212766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.335557</td>\n",
       "      <td>0.628287</td>\n",
       "      <td>0.036156</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.191489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.411049</td>\n",
       "      <td>0.560474</td>\n",
       "      <td>0.028477</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>0.285907</td>\n",
       "      <td>0.144760</td>\n",
       "      <td>0.569333</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.680851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0.592421</td>\n",
       "      <td>0.221579</td>\n",
       "      <td>0.186000</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.573267</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.425725</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.340426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.278421</td>\n",
       "      <td>0.482213</td>\n",
       "      <td>0.239366</td>\n",
       "      <td>0.191489</td>\n",
       "      <td>0.617021</td>\n",
       "      <td>0.191489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.279701</td>\n",
       "      <td>0.335261</td>\n",
       "      <td>0.385038</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.170213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           y1        y2        y3       y_1       y_2       y_3\n",
       "0    0.231543  0.205917  0.562540  0.275000  0.000000  0.725000\n",
       "1    0.392368  0.269764  0.337868  0.650000  0.175000  0.175000\n",
       "2    0.443386  0.515899  0.040715  0.350000  0.650000  0.000000\n",
       "3    0.029966  0.591738  0.378297  0.000000  0.675000  0.325000\n",
       "4    0.456095  0.181239  0.362666  0.450000  0.000000  0.550000\n",
       "5    0.242409  0.592754  0.164837  0.100000  0.875000  0.025000\n",
       "6    0.043063  0.406220  0.550717  0.150000  0.775000  0.075000\n",
       "7    0.266536  0.318728  0.414736  0.200000  0.325000  0.475000\n",
       "8    0.686134  0.156928  0.156938  0.650000  0.000000  0.350000\n",
       "9    0.481345  0.007312  0.511343  0.125000  0.000000  0.875000\n",
       "10   0.411049  0.560474  0.028477  0.145833  0.833333  0.020833\n",
       "11   0.278421  0.482213  0.239366  0.625000  0.250000  0.125000\n",
       "12   0.182504  0.344171  0.473326  0.104167  0.333333  0.562500\n",
       "13   0.592421  0.221579  0.186000  0.541667  0.312500  0.145833\n",
       "14   0.485555  0.161659  0.352787  0.291667  0.062500  0.645833\n",
       "15   0.279701  0.335261  0.385038  0.229167  0.416667  0.354167\n",
       "16   0.448748  0.205368  0.345883  0.437500  0.354167  0.208333\n",
       "17   0.042217  0.631844  0.325939  0.250000  0.250000  0.500000\n",
       "18   0.741335  0.099506  0.159158  0.541667  0.020833  0.437500\n",
       "19   0.282196  0.170358  0.547446  0.812500  0.062500  0.125000\n",
       "20   0.573267  0.001008  0.425725  0.270833  0.083333  0.645833\n",
       "21   0.285907  0.144760  0.569333  0.541667  0.062500  0.395833\n",
       "22   0.347581  0.151866  0.500553  0.330000  0.240000  0.430000\n",
       "23   0.410772  0.560348  0.028880  0.110000  0.810000  0.070000\n",
       "24   0.446779  0.386421  0.166800  0.800000  0.150000  0.060000\n",
       "25   0.181623  0.344440  0.473937  0.150000  0.170000  0.690000\n",
       "26   0.592421  0.221579  0.186000  0.610000  0.350000  0.040000\n",
       "27   0.485555  0.161659  0.352787  0.330000  0.280000  0.390000\n",
       "28   0.280048  0.336158  0.383794  0.440000  0.260000  0.300000\n",
       "29   0.449181  0.205641  0.345179  0.590000  0.130000  0.280000\n",
       "30   0.041589  0.628061  0.330349  0.170000  0.200000  0.630000\n",
       "31   0.742360  0.098499  0.159141  0.650000  0.040000  0.310000\n",
       "32   0.280363  0.175847  0.543789  0.520000  0.190000  0.300000\n",
       "33   0.574079  0.000968  0.424953  0.500000  0.170000  0.330000\n",
       "34   0.283973  0.148289  0.567737  0.240000  0.170000  0.590000\n",
       "35   0.411049  0.560474  0.028477  0.431034  0.568965  0.000000\n",
       "36   0.278421  0.482213  0.239366  0.258621  0.534483  0.206897\n",
       "37   0.311129  0.387039  0.301832  0.258621  0.482759  0.258621\n",
       "38   0.370784  0.221425  0.407791  0.500000  0.172414  0.327586\n",
       "39   0.592421  0.221579  0.186000  0.775862  0.172414  0.051724\n",
       "40   0.485555  0.161659  0.352787  0.482759  0.068966  0.431034\n",
       "41   0.279701  0.335261  0.385038  0.534483  0.137931  0.327586\n",
       "42   0.454607  0.233683  0.311709  0.465517  0.155172  0.379310\n",
       "43   0.182504  0.344171  0.473326  0.051724  0.241379  0.706897\n",
       "44   0.448748  0.205368  0.345883  0.551724  0.206897  0.241379\n",
       "45   0.308948  0.450920  0.240131  0.258621  0.603448  0.228571\n",
       "46   0.741335  0.099506  0.159158  0.603448  0.051724  0.344828\n",
       "47   0.282196  0.170358  0.547446  0.224138  0.172414  0.603448\n",
       "48   0.573267  0.001008  0.425725  0.568965  0.034483  0.396552\n",
       "49   0.285907  0.144760  0.569333  0.293103  0.034483  0.672414\n",
       "50   0.602258  0.308756  0.088986  0.789116  0.047619  0.163265\n",
       "51   0.245639  0.241903  0.512459  0.006803  0.068027  0.925170\n",
       "52   0.411049  0.560474  0.028477  0.442177  0.517007  0.040816\n",
       "53   0.247404  0.508040  0.244555  0.142857  0.802721  0.054422\n",
       "54   0.830847  0.054034  0.115119  0.911565  0.027211  0.061224\n",
       "55   0.285907  0.144760  0.569333  0.217687  0.034014  0.748299\n",
       "56   0.576628  0.192186  0.231186  0.843537  0.034014  0.122449\n",
       "57   0.254107  0.727788  0.018105  0.027211  0.945578  0.027211\n",
       "58   0.592421  0.221579  0.186000  0.816327  0.129252  0.054422\n",
       "59   0.284550  0.188365  0.527085  0.020408  0.047619  0.931973\n",
       "60   0.481115  0.502597  0.016288  0.666667  0.312925  0.020408\n",
       "61   0.573267  0.001008  0.425725  0.653061  0.088435  0.258503\n",
       "62   0.602591  0.178472  0.218937  0.693878  0.102041  0.204082\n",
       "63   0.048255  0.710317  0.241427  0.027211  0.870748  0.102041\n",
       "64   0.278421  0.482213  0.239366  0.292517  0.523810  0.183673\n",
       "65   0.444206  0.236482  0.319312  0.748299  0.149660  0.102041\n",
       "66   0.204632  0.260015  0.535353  0.027211  0.068027  0.904762\n",
       "67   0.279701  0.335261  0.385038  0.489796  0.190476  0.319728\n",
       "68   0.335557  0.628287  0.036156  0.197279  0.659864  0.142857\n",
       "69   0.102947  0.354474  0.542579  0.013605  0.074830  0.911565\n",
       "70   0.325120  0.601401  0.073479  0.346667  0.653333  0.000000\n",
       "71   0.023402  0.549526  0.427073  0.013333  0.480000  0.506667\n",
       "72   0.592421  0.221579  0.186000  0.853333  0.106667  0.040000\n",
       "73   0.595773  0.346056  0.058171  0.826667  0.160000  0.013333\n",
       "74   0.602591  0.178472  0.218937  0.706667  0.053333  0.240000\n",
       "75   0.014257  0.980008  0.005735  0.013333  0.546667  0.440000\n",
       "76   0.362044  0.000578  0.637378  0.480000  0.013333  0.506667\n",
       "77   0.190147  0.404341  0.405512  0.000000  0.560000  0.440000\n",
       "78   0.462945  0.244622  0.292433  0.586667  0.360000  0.053333\n",
       "79   0.137816  0.277193  0.584992  0.000000  0.160000  0.840000\n",
       "80   0.335557  0.628287  0.036156  0.226667  0.693333  0.080000\n",
       "81   0.631657  0.086628  0.281715  0.720000  0.000000  0.280000\n",
       "82   0.370784  0.221425  0.407791  0.520000  0.186667  0.293333\n",
       "83   0.165873  0.365628  0.468499  0.013333  0.573333  0.413333\n",
       "84   0.448150  0.435468  0.116382  0.400000  0.586667  0.013333\n",
       "85   0.711729  0.257204  0.031067  0.765957  0.234043  0.000000\n",
       "86   0.126751  0.426659  0.446591  0.000000  0.617021  0.382979\n",
       "87   0.554718  0.053493  0.391789  0.276596  0.000000  0.723404\n",
       "88   0.247404  0.508040  0.244555  0.191489  0.744681  0.063830\n",
       "89   0.576628  0.192186  0.231186  0.829787  0.021277  0.148936\n",
       "90   0.284550  0.188365  0.527085  0.021277  0.042553  0.936170\n",
       "91   0.602591  0.178472  0.218937  0.680851  0.127660  0.191489\n",
       "92   0.444206  0.236482  0.319312  0.787234  0.148936  0.063830\n",
       "93   0.245639  0.241903  0.512459  0.000000  0.085106  0.914894\n",
       "94   0.830847  0.054034  0.115119  0.936170  0.000000  0.063830\n",
       "95   0.254107  0.727788  0.018105  0.000000  0.978723  0.021277\n",
       "96   0.481115  0.502597  0.016288  0.638298  0.340426  0.021277\n",
       "97   0.048255  0.710317  0.241427  0.021277  0.872340  0.106383\n",
       "98   0.204632  0.260015  0.535353  0.000000  0.063830  0.936170\n",
       "99   0.102947  0.354474  0.542579  0.000000  0.063830  0.936170\n",
       "100  0.602258  0.308756  0.088986  0.765957  0.021277  0.212766\n",
       "101  0.335557  0.628287  0.036156  0.191489  0.617021  0.191489\n",
       "102  0.411049  0.560474  0.028477  0.425532  0.531915  0.042553\n",
       "103  0.285907  0.144760  0.569333  0.297872  0.021277  0.680851\n",
       "104  0.592421  0.221579  0.186000  0.808511  0.191489  0.000000\n",
       "105  0.573267  0.001008  0.425725  0.595745  0.063830  0.340426\n",
       "106  0.278421  0.482213  0.239366  0.191489  0.617021  0.191489\n",
       "107  0.279701  0.335261  0.385038  0.531915  0.297872  0.170213"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "compare = pd.DataFrame(np.concatenate((y.eval(feed_dict={x: inputs_tests, y_: target_tests, keep_prob: 1.0}),\n",
    "                            y_.eval(feed_dict={x: inputs_tests, y_: target_tests, keep_prob: 1.0})),axis=1))\n",
    "compare.columns = [\"y1\",\"y2\",\"y3\",\"y_1\",\"y_2\",\"y_3\"]\n",
    "compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
